<!DOCTYPE html>
<html>
<head>
<title>readme.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="%E5%AF%B9%E6%AF%94%E8%AE%BA%E6%96%87%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E7%BB%9F%E8%AE%A1">对比论文实验结果统计</h1>
<h2 id="icassp2023">ICASSP2023</h2>
<h3 id="audio">audio</h3>
<pre><code>IEMOCAP
utterance: 5531
conversations: 151
l-o-se:leave-one-session-out  
l-o-sp:leave-one-speaker-out
</code></pre>
<table>
<thead>
<tr>
<th>Method</th>
<th>UA</th>
<th>WA</th>
<th>split</th>
<th>level</th>
<th>Features SSL</th>
<th>url</th>
<th>remark</th>
</tr>
</thead>
<tbody>
<tr>
<td>MSMSER(only audio)</td>
<td>64.9/63.4</td>
<td>63.2/62.9</td>
<td>l-o-se</td>
<td>uttrance</td>
<td>HuBERT &amp; MPNet</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10096709">Exploring Complementary Features in Multi-Modal Speech Emotion Recognition </a></td>
<td></td>
</tr>
<tr>
<td>DST</td>
<td>73.6</td>
<td>71.8</td>
<td>l-o-se</td>
<td>uttrance</td>
<td>WavLM</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10096966">DST: Deformable Speech Transformer for Emotion Recognition</a></td>
<td></td>
</tr>
<tr>
<td>SMW_CAT</td>
<td>74.25</td>
<td>73.8</td>
<td>l-o-sp</td>
<td>uttrance</td>
<td>Wav2vec2 &amp; MFCC</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10095777">Multiple Acoustic Features Speech Emotion Recognition Using Cross-Attention Transformer</a></td>
<td>不可参考</td>
</tr>
<tr>
<td>DCW + TsPA</td>
<td>72.17/74.26</td>
<td>72.08/73.18</td>
<td>l-o-se/l-o-sp</td>
<td>uttrance</td>
<td>Wav2vec2 &amp; MFCC</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10095588">Speech Emotion Recognition Via Two-Stream Pooling Attention With Discriminative Channel Weighting</a></td>
<td></td>
</tr>
<tr>
<td>LabelAdaptiveMixup-SER</td>
<td>76.04</td>
<td>75.37</td>
<td>l-o-se</td>
<td>uttrance</td>
<td>HuBERT-Large+<em><strong>Finetune</strong></em></td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10095611">Learning Robust Self-Attention Features for Speech Emotion Recognition with Label-Adaptive Mixup</a></td>
<td>结果挺严谨，且有源码，建议参考</td>
</tr>
<tr>
<td>DKDFMH</td>
<td>77.0</td>
<td>79.1</td>
<td>randomly train:80%, tret:20%</td>
<td>uttrance <em><strong>存疑</strong></em></td>
<td>logF-Bank</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10095045">Hierarchical Network with Decoupled Knowledge Distillation for Speech Emotion Recognition</a></td>
<td>存疑，需要一起讨论</td>
</tr>
<tr>
<td>-</td>
<td>F1=71</td>
<td>.-</td>
<td>l-o-se</td>
<td>uttrance</td>
<td>WavLM Large+<em><strong>Finetune</strong></em></td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10096578">Domain Adaptation without Catastrophic Forgetting on a Small-Scale Partially-Labeled Corpus for Speech Emotion Recognition</a></td>
<td>不建议对比</td>
</tr>
<tr>
<td>-</td>
<td>73.86/701./75.60</td>
<td>-</td>
<td>l-o-se</td>
<td>uttrance</td>
<td>HuBERT large/Wav2vec 2.0/WavLM Large <em><strong>all Finetune</strong></em></td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10094673">Speech-Based Emotion Recognition with Self-Supervised Models Using Attentive Channel-Wise Correlations and Label Smoothing</a></td>
<td></td>
</tr>
<tr>
<td>TIM-Net</td>
<td>72.5</td>
<td>-</td>
<td>l-o-sp</td>
<td>uttrance</td>
<td>MFCC</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10096370">Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition</a></td>
<td>有源码，给了特征，可信度较高，可参考</td>
</tr>
<tr>
<td>TFA+TFW+BCNN</td>
<td>79.07</td>
<td>81.57</td>
<td>l-o-se</td>
<td>uttrance</td>
<td>MFCC</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10095260">Speech Emotion Recognition Based on Low-Level Auto-Extracted Time-Frequency Features</a></td>
<td>结果高的离谱</td>
</tr>
<tr>
<td>cFW-VCs</td>
<td>CCC = 63.6/71.9</td>
<td></td>
<td>l-o-se</td>
<td>uttrance</td>
<td>LLDs/Wav2Vec2-large</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10096861">Role of Lexical Boundary Information in Chunk-Level Segmentation for Speech Emotion Recognition</a></td>
<td>Chunk-level 估计不能用</td>
</tr>
<tr>
<td>P-TAPT</td>
<td>74.3</td>
<td>-</td>
<td>l-o-se</td>
<td>uttrance</td>
<td>LLDs/Wav2Vec2-finetune</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10095036">Exploring Wav2vec 2.0 Fine Tuning for Improved Speech Emotion Recognition</a></td>
<td></td>
</tr>
<tr>
<td>DWFormer</td>
<td>73.9</td>
<td>72.3</td>
<td>l-o-se</td>
<td>uttrance</td>
<td>WavLm-large</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10094651">DWFormer: Dynamic Window Transformer for Speech Emotion Recognition</a></td>
<td></td>
</tr>
<tr>
<td>ShiftCNN/ShiftLSTM/Shiftformer</td>
<td>74.5/74.7/74.8</td>
<td>-</td>
<td>l-o-se</td>
<td>uttrance</td>
<td>wav2vec2+<em><strong>finetune</strong></em>+Hubert+<em><strong>finetune</strong></em></td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10095193">Mingling or Misalignment? Temporal Shift for Speech Emotion Recognition with Pre-Trained Representations</a></td>
<td>这篇文章对是否finetune分开进行了讨论，给了源码，参考意义较大</td>
</tr>
<tr>
<td>同上</td>
<td>72.8/76/.72.7</td>
<td>71.9/69.8/72.1</td>
<td>同上</td>
<td>同上</td>
<td>wav2vec2</td>
<td>同上</td>
<td></td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10096808">Designing and Evaluating Speech Emotion Recognition Systems: A Reality Check Case Study with IEMOCAP</a></td>
<td>综述性文章，里面结果可以做参考</td>
</tr>
<tr>
<td>EMix-S</td>
<td>71.85</td>
<td>77.63</td>
<td>speaker-dependent+5fold</td>
<td>uttrance</td>
<td>log-Mel magnitude spectrogram</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10096789">EMIX: A Data Augmentation Method for Speech Emotion Recognition</a></td>
<td>5fold交叉验证方法待讨论</td>
</tr>
</tbody>
</table>
<h2 id="icassp-2022">ICASSP 2022</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>UA</th>
<th>WA</th>
<th>split</th>
<th>level</th>
<th>SSL</th>
<th>url</th>
<th>remark</th>
</tr>
</thead>
<tbody>
<tr>
<td>GLAM</td>
<td>73.90</td>
<td>73.70</td>
<td>0.8train: 0.2test</td>
<td>utterance</td>
<td>MFCC</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9747517">Speech Emotion Recognition with Global-Aware Fusion on Multi-Scale Feature Representation</a></td>
<td>improv, script 分开统计</td>
</tr>
<tr>
<td>DAE+Linear-SVM</td>
<td>52.09</td>
<td>-</td>
<td>l-o-se</td>
<td>utterance</td>
<td>eGeMAPS</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9746450">Towards Transferable Speech Emotion Representation: On Loss Functions for Cross-Lingual Latent Representations</a></td>
<td></td>
</tr>
<tr>
<td>CNN_SeqCap</td>
<td>56.91</td>
<td>70.54</td>
<td>l-o-se</td>
<td>utterance</td>
<td>spectrograms</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9746155">Neural Architecture Search for Speech Emotion Recognition</a></td>
<td></td>
</tr>
<tr>
<td>LIGHT-SERNET</td>
<td>70.76</td>
<td>70.23</td>
<td>l-o-sp</td>
<td>utterance</td>
<td>MFCCs</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9746679">LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition</a></td>
<td>improv, script 分开统计</td>
</tr>
<tr>
<td>ECAPA</td>
<td>77.76</td>
<td>77.36</td>
<td>l-o-se</td>
<td>utterance</td>
<td>HuBERT+W2V2</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9747870">Speech Emotion Recognition Using Self-Supervised Features</a></td>
<td></td>
</tr>
<tr>
<td>CKE-Net</td>
<td>66.5</td>
<td>-</td>
<td>4810/1000/1523(7433)</td>
<td>conversation</td>
<td>HuBERT+W2V2</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9746909">A Commonsense Knowledge Enhanced Network with Retrospective Loss for Emotion Recognition in Spoken Dialog</a></td>
<td>IEMCAP7433</td>
</tr>
<tr>
<td>TAP</td>
<td>74.3</td>
<td>-</td>
<td>l-o-se</td>
<td>utterance</td>
<td>HuBERT Large</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9747460">Speaker Normalization for Self-Supervised Speech Emotion Recognition</a></td>
<td></td>
</tr>
<tr>
<td>Co-Attention</td>
<td>71.05</td>
<td>69.80</td>
<td>l-o-se</td>
<td>utterance</td>
<td>MFCC+wav2vec2+spectrograms</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9747095">Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information</a></td>
<td></td>
</tr>
<tr>
<td>Co-Attention</td>
<td>72.70</td>
<td>71.64</td>
<td>l-o-sp</td>
<td>utterance</td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9747095">Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information</a></td>
<td></td>
</tr>
<tr>
<td>averaged models</td>
<td>75.2</td>
<td>-</td>
<td>l-o-se</td>
<td>utterance</td>
<td>HuBERT Large finetuned</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9747756">Towards A Common Speech Analysis Engine</a></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="mutil-modal">Mutil-Modal</h3>
<h2 id="datasets">datasets</h2>
<table>
<thead>
<tr>
<th>name</th>
<th>uttrance</th>
<th>conversation</th>
</tr>
</thead>
<tbody>
<tr>
<td>IEMOCAP7433</td>
<td>7433</td>
<td>151</td>
</tr>
<tr>
<td>IEMOCAP5531</td>
<td>5531</td>
<td>151</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Method</th>
<th>F1-Weighted</th>
<th>UA</th>
<th>WA</th>
<th>split</th>
<th>level</th>
<th>SSL</th>
<th>url</th>
<th>dataset</th>
<th>remark</th>
</tr>
</thead>
<tbody>
<tr>
<td>MSRFG</td>
<td>71.60</td>
<td>-</td>
<td>-</td>
<td>leave last 20 conversations for test</td>
<td>dialog</td>
<td>a: Wav2vec2 + t: Roberta-Large</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10094596">Multi-Scale Receptive Field Graph Model for Emotion Recognition in Conversations</a></td>
<td>IEMACP7433</td>
<td>内含多个结果可参考</td>
</tr>
<tr>
<td>SMCN</td>
<td>-</td>
<td>77.6</td>
<td>75.6</td>
<td>l-o-se</td>
<td>conversation</td>
<td>a: &amp; t:</td>
<td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9747859">Multi-Modal Emotion Recognition with Self-Guided Modality Calibration</a></td>
<td>IEMACP5531</td>
<td></td>
</tr>
<tr>
<td>SMCN</td>
<td>62.3</td>
<td>64.9</td>
<td>-</td>
<td>conversation</td>
<td>-</td>
<td>-</td>
<td>MELD</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="result">result</h2>
<h3 id="tim-netser-mfcc-features--tfcnn">TIM-Net_SER MFCC features + TFCNN</h3>
<p>/data/ADGCNForEMC/data/featuresFromPapers/IEMOCAP.npy 根据label判断顺序，基本性能, (librosa toolbox)
使用<a href="https://github.com/Jiaxin-Ye/TIM-Net_SER">源码</a>重新提取
<strong>result(best)TFCNN</strong></p>
<table>
<thead>
<tr>
<th>num</th>
<th>session</th>
<th>WA</th>
<th>UA</th>
</tr>
</thead>
<tbody>
<tr>
<td>1085</td>
<td>session1</td>
<td>53.06</td>
<td>55.04</td>
</tr>
<tr>
<td>1023</td>
<td>session2</td>
<td>55.43</td>
<td>55.10</td>
</tr>
<tr>
<td>1151</td>
<td>session3</td>
<td>54.56</td>
<td>53.91</td>
</tr>
<tr>
<td>1031</td>
<td>session4</td>
<td>53.06</td>
<td>55.04</td>
</tr>
<tr>
<td>1241</td>
<td>session5</td>
<td>52.62</td>
<td>54.73</td>
</tr>
<tr>
<td>5531</td>
<td>mean</td>
<td>53.75*(53.71)</td>
<td>54.76(54.75)</td>
</tr>
</tbody>
</table>
<p>*<em>:weighted</em></p>
<h3 id="wavlmlarge">wavlm_large</h3>
<p>wavlm_large + CNNSelfAttention 5fold</p>
<table>
<thead>
<tr>
<th>num</th>
<th>session</th>
<th>WA</th>
<th>UA</th>
</tr>
</thead>
<tbody>
<tr>
<td>1085</td>
<td>session1</td>
<td>72.63</td>
<td>73.01</td>
</tr>
<tr>
<td>1023</td>
<td>session2</td>
<td>78.89</td>
<td>847.</td>
</tr>
<tr>
<td>1151</td>
<td>session3</td>
<td>74.72</td>
<td>74.44</td>
</tr>
<tr>
<td>1031</td>
<td>session4</td>
<td>78.66</td>
<td>77.89</td>
</tr>
<tr>
<td>1241</td>
<td>session5</td>
<td>73.01</td>
<td>73.95</td>
</tr>
<tr>
<td>5531</td>
<td>mean</td>
<td>75.43(75.58)</td>
<td>75.86(75.95)</td>
</tr>
</tbody>
</table>
<p>wavlm_large + CNNSelfAttention + dialogGCN 5fold</p>
<table>
<thead>
<tr>
<th>num</th>
<th>session</th>
<th>WA</th>
<th>UA</th>
</tr>
</thead>
<tbody>
<tr>
<td>1085</td>
<td>session1</td>
<td>75.48</td>
<td>77.31</td>
</tr>
<tr>
<td>1023</td>
<td>session2</td>
<td>82.21</td>
<td>81.82</td>
</tr>
<tr>
<td>1151</td>
<td>session3</td>
<td>77.50</td>
<td>76.89</td>
</tr>
<tr>
<td>1031</td>
<td>session4</td>
<td>75.85</td>
<td>73.31</td>
</tr>
<tr>
<td>1241</td>
<td>session5</td>
<td>79.94</td>
<td>79.13</td>
</tr>
<tr>
<td>5531</td>
<td>mean</td>
<td>78.21*(78.20)</td>
<td>77.72(77.69)</td>
</tr>
</tbody>
</table>
<p>*<em>:weighted</em>
wavlm_large + CNNSelfAttention + GCNII 5fold</p>
<table>
<thead>
<tr>
<th>num</th>
<th>session</th>
<th>WA</th>
<th>UA</th>
</tr>
</thead>
<tbody>
<tr>
<td>1085</td>
<td>session1</td>
<td>77.70</td>
<td>80.36</td>
</tr>
<tr>
<td>1023</td>
<td>session2</td>
<td>84.07</td>
<td>85.04</td>
</tr>
<tr>
<td>1151</td>
<td>session3</td>
<td>78.80</td>
<td>78.63</td>
</tr>
<tr>
<td>1031</td>
<td>session4</td>
<td>78.95</td>
<td>78.05</td>
</tr>
<tr>
<td>1241</td>
<td>session5</td>
<td>75.75</td>
<td>76.08</td>
</tr>
<tr>
<td>5531</td>
<td>mean</td>
<td>78.90*(79.05)</td>
<td>79.47(79.63)</td>
</tr>
</tbody>
</table>
<p>*<em>:weighted</em></p>
<p>wavlm_large + CNNSelfAttention + ADGCN 5fold</p>
<table>
<thead>
<tr>
<th>num</th>
<th>session</th>
<th>WA</th>
<th>UA</th>
</tr>
</thead>
<tbody>
<tr>
<td>1085</td>
<td>session1</td>
<td>78.62</td>
<td>80.95</td>
</tr>
<tr>
<td>1023</td>
<td>session2</td>
<td>83.28</td>
<td>84.47</td>
</tr>
<tr>
<td>1151</td>
<td>session3</td>
<td>79.32</td>
<td>79.02</td>
</tr>
<tr>
<td>1031</td>
<td>session4</td>
<td>79.44</td>
<td>76.85</td>
</tr>
<tr>
<td>1241</td>
<td>session5</td>
<td>77.60</td>
<td>76.61</td>
</tr>
<tr>
<td>5531</td>
<td>mean</td>
<td>79.55*(79.65)</td>
<td>79.46(79.58)</td>
</tr>
</tbody>
</table>
<p>*<em>:weighted</em></p>
<p>wavlm_large + CNNSelfAttention + CausalDiaModel</p>
<p>| | | | | |</p>
<!-- wavlm_large + CNNSelfAttention(f1) + CausalDiaModel + ADGCN 5fold

| num | session | WA | UA |  
| --- | --- | ---| --- |
| 1085 | session1 | 76.13 | 80.07 |
| 1023 | session2 | 84.56 | 85.70 |
| 1151 | session3 | 80.19 | 80.01 |
| 1031 | session4 | 79.44 | 76.48 |
| 1241 | session5 | 81.63 | 80.97 |
| 5531 | mean | 80.39(80.39) | 80.63(80.65) |
**:weighted* -->
<p>wavlm_large + CNNSelfAttention(f1) + CausalDiaModel + ADGCN 5fold</p>
<table>
<thead>
<tr>
<th>num</th>
<th>session</th>
<th>WA</th>
<th>UA</th>
</tr>
</thead>
<tbody>
<tr>
<td>1085</td>
<td>session1</td>
<td>76.13</td>
<td>80.07</td>
</tr>
<tr>
<td>1023</td>
<td>session2</td>
<td>84.56</td>
<td>85.70</td>
</tr>
<tr>
<td>1151</td>
<td>session3</td>
<td>80.19</td>
<td>80.01</td>
</tr>
<tr>
<td>1031</td>
<td>session4</td>
<td>79.44</td>
<td>76.48</td>
</tr>
<tr>
<td>1241</td>
<td>session5</td>
<td>81.63</td>
<td>80.97</td>
</tr>
<tr>
<td>5531</td>
<td>mean</td>
<td>80.39(80.39)</td>
<td>80.63(80.65)</td>
</tr>
</tbody>
</table>
<p>*<em>:weighted</em></p>
<ol start="3">
<li>2022 icassp 总结, 语音，多模态   TAC, IEEE trans</li>
</ol>
<h3 id="plan">plan</h3>
<p>Deep graph For context application</p>
<ol>
<li>
<p>绪论(背景, 国内研究现状, 问题分析)</p>
</li>
<li>
<p>GNN相关方法概述(GNN, DeepGNN, 问题+解决方案) + context information fusion? &amp; model?</p>
</li>
<li>
<p>Deep graph 理论 MAGCN</p>
</li>
<li>
<p>DiaMAGCN: MAGCN + Dialog  for 对话  audio + dialog: global, context,</p>
</li>
<li>
<p>MAGCN + Dialog + mutilModal  for 多模态 vat + dialog:</p>
</li>
</ol>
<h3 id="magcn--dialog-plan">MAGCN + Dialog plan</h3>
<ol>
<li>
<p>SPL 5页</p>
</li>
<li>
<p><strong>local context feature</strong> + ADGCN</p>
</li>
<li>
<p>实验：</p>
<ol>
<li><strong>对比结果 大表(DialogGCN, DialogRNN)</strong></li>
<li>diff layers</li>
<li>消融实验:
<ol>
<li>local context feature with / without</li>
<li>single layer / ADGCN</li>
</ol>
</li>
</ol>
</li>
<li>
<p>参考文献
IEMOCAP 2023, 2022, 2021
SPL
DialogGCN 相关</p>
</li>
</ol>
<h2 id="%E7%B4%A7%E6%80%A5">紧急</h2>
<ol>
<li>
<p><strong>local context feature</strong> 测试性能</p>
</li>
<li>
<p>MELD的实验</p>
</li>
<li>
<p>找文献</p>
</li>
<li>
<p>MELD 再优化</p>
</li>
<li>
<p>SPL 再找找文献</p>
</li>
<li>
<p>CONCAT [context, origin]</p>
</li>
</ol>

</body>
</html>
